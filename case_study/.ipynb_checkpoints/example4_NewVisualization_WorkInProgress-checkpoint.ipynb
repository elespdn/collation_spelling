{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add a property to the token for normalized forms to be used not in the alignment, but for interpretation in the analysis stage and then in the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. No normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "W1 = open( \"data/example4/W1.txt\", encoding='utf-8' ).read()\n",
    "W2 = open( \"data/example4/W2.txt\", encoding='utf-8' ).read()\n",
    "W3 = open( \"data/example4/W3.txt\", encoding='utf-8' ).read()\n",
    "W4 = open( \"data/example4/W4.txt\", encoding='utf-8' ).read()\n",
    "collation.add_plain_witness( \"W1\", W1 )\n",
    "collation.add_plain_witness( \"W2\", W2 )\n",
    "collation.add_plain_witness( \"W3\", W3 )\n",
    "collation.add_plain_witness( \"W4\", W4 )\n",
    "table = collate(collation, output='html2', segmentation=False)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Dictionary\n",
    "\n",
    "This method requires the manual creation of a dictionary. In this example, the dictionary is built with **three columns**: the first for the **original form**, the second for the **normalized form** to be used during the **alignment**, the third for the **normalized form** to be used in the **interpretation**, after the alignment and before the visualisation.\n",
    "The first column must have a value, while the second and third columns may stay empty (later addition: if it's empty, does it take the t value by default???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"table\": [[[{\"_sigil\": \"W1\", \"_token_array_position\": 0, \"n\": \"Lors\", \"p\": \"Lors\", \"t\": \"Lors \"}], null, [{\"_sigil\": \"W1\", \"_token_array_position\": 1, \"n\": \"conte\", \"p\": \"conte\", \"t\": \"conte \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 2, \"n\": \"li\", \"p\": \"li\", \"t\": \"li \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 3, \"n\": \"rois\", \"p\": \"rois\", \"t\": \"rois \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 4, \"n\": \"a\", \"p\": \"a\", \"t\": \"a \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 5, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 6, \"n\": \"reine\", \"p\": \"reine\", \"t\": \"reine \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 7, \"n\": \"coment\", \"p\": \"coment\", \"t\": \"coment \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 8, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 9, \"n\": \"dame\", \"p\": \"dame\", \"t\": \"dame \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 10, \"n\": \"del\", \"p\": \"del\", \"t\": \"del \"}], [{\"_sigil\": \"W1\", \"_token_array_position\": 11, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]], [[{\"_sigil\": \"W2\", \"_token_array_position\": 13, \"n\": \"Lors\", \"p\": \"Lors\", \"t\": \"Lors \"}], null, [{\"_sigil\": \"W2\", \"_token_array_position\": 14, \"n\": \"conte\", \"p\": \"conte\", \"t\": \"conte \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 15, \"n\": \"li\", \"p\": \"li\", \"t\": \"li \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 16, \"n\": \"rois\", \"p\": \"rois\", \"t\": \"rois \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 17, \"n\": \"a\", \"p\": \"a\", \"t\": \"a \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 18, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 19, \"n\": \"reine\", \"p\": \"reine\", \"t\": \"reine \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 20, \"n\": \"coment\", \"p\": \"coment\", \"t\": \"coment \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 21, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 22, \"n\": \"dame\", \"p\": \"dame\", \"t\": \"dame \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 23, \"n\": \"del\", \"p\": \"del\", \"t\": \"del \"}], [{\"_sigil\": \"W2\", \"_token_array_position\": 24, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]], [[{\"_sigil\": \"W3\", \"_token_array_position\": 26, \"n\": \"Lors\", \"p\": \"Lors\", \"t\": \"Lors \"}], null, [{\"_sigil\": \"W3\", \"_token_array_position\": 27, \"n\": \"conte\", \"p\": \"conte\", \"t\": \"conte \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 28, \"n\": \"li\", \"p\": \"li\", \"t\": \"li \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 29, \"n\": \"rois\", \"p\": \"rois\", \"t\": \"rois \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 30, \"n\": \"a\", \"p\": \"a\", \"t\": \"a \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 31, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 32, \"n\": \"reine\", \"p\": \"reine\", \"t\": \"roine \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 33, \"n\": \"coment\", \"p\": \"coment\", \"t\": \"coment \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 34, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 35, \"n\": \"dame\", \"p\": \"dame\", \"t\": \"dame \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 36, \"n\": \"del\", \"p\": \"del\", \"t\": \"del \"}], [{\"_sigil\": \"W3\", \"_token_array_position\": 37, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]], [[{\"_sigil\": \"W4\", \"_token_array_position\": 39, \"n\": \"Adonc\", \"p\": \"Adonc\", \"t\": \"Adonc \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 40, \"n\": \"li\", \"p\": \"li\", \"t\": \"li \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 41, \"n\": \"conte\", \"p\": \"conte\", \"t\": \"conte \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 42, \"n\": \"li\", \"p\": \"li\", \"t\": \"li \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 43, \"n\": \"rois\", \"p\": \"rois\", \"t\": \"rois \"}], null, null, null, [{\"_sigil\": \"W4\", \"_token_array_position\": 44, \"n\": \"coment\", \"p\": \"coment\", \"t\": \"comment \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 45, \"n\": \"la\", \"p\": \"la\", \"t\": \"la \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 46, \"n\": \"dame\", \"p\": \"dame\", \"t\": \"dame \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 47, \"n\": \"\", \"p\": \"del\", \"t\": \"du \"}], [{\"_sigil\": \"W4\", \"_token_array_position\": 48, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]]], \"witnesses\": [\"W1\", \"W2\", \"W3\", \"W4\"]}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from collatex import *\n",
    "collation = Collation()\n",
    "\n",
    "# Create the dictionary (here 'dictionary_norm.csv') with three columns: the first for the original form (t), the second for the normalized form to be used during the alignment (n), the third for the normalized form to be used in the interpretation (p), after the alignment and before the visualisation. The first column must have a value, while the second and third columns may stay empty.\n",
    "Normit = {}\n",
    "with open('dictionary_norm.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'NormalisedAlignment', 'NormalisedInterpretation'])\n",
    "    for row in reader:\n",
    "        Normit[row['Original']]= row['NormalisedAlignment']\n",
    "        \n",
    "NormitInterpretation = {}\n",
    "with open('dictionary_norm.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'NormalisedAlignment', 'NormalisedInterpretation'])\n",
    "    for row in reader:\n",
    "        NormitInterpretation[row['Original']]= row['NormalisedInterpretation']\n",
    "\n",
    "from collatex.core_classes import WordPunctuationTokenizer\n",
    "tokenizer = WordPunctuationTokenizer()\n",
    "\n",
    "#read in the witnesses  from your file system \n",
    "W1 = open( \"data/example4/W1.txt\", encoding='utf-8' ).read()\n",
    "W2 = open( \"data/example4/W2.txt\", encoding='utf-8' ).read()\n",
    "W3 = open( \"data/example4/W3.txt\", encoding='utf-8' ).read()\n",
    "W4 = open( \"data/example4/W4.txt\", encoding='utf-8' ).read()\n",
    "\n",
    "# build a function to tokenize and to normalize by replace keys to be found in the dictionary by the corresponding values \n",
    "def tokennormalizer(witness) :\n",
    "    tokens_as_strings = tokenizer.tokenize(witness)\n",
    "    list = []\n",
    "    for token_string in tokens_as_strings:\n",
    "        normversion = re.sub(r'\\s+$',\"\", token_string)\n",
    "        replaceversion = Normit.get(normversion,normversion)\n",
    "        token_norm = NormitInterpretation.get(normversion,normversion)\n",
    "        list.append({'t':token_string, 'n':replaceversion, 'p':token_norm})\n",
    "    return(list)\n",
    "\n",
    "tokens_W1 = tokennormalizer(W1) \n",
    "tokens_W2 = tokennormalizer(W2) \n",
    "tokens_W3 = tokennormalizer(W3) \n",
    "tokens_W4 = tokennormalizer(W4) \n",
    "#Print to check what's in the properties; can be deleted once we can visualize it. Can check also in the collation with json output.\n",
    "##print(tokens_W1, tokens_W2, tokens_W3, tokens_W4)\n",
    " \n",
    "witness_W1 = { \"id\": \"W1\", \"tokens\":tokens_W1 }\n",
    "witness_W2 = { \"id\": \"W2\", \"tokens\":tokens_W2 }\n",
    "witness_W3 = { \"id\": \"W3\", \"tokens\":tokens_W3 }\n",
    "witness_W4 = { \"id\": \"W4\", \"tokens\":tokens_W4 }\n",
    "\n",
    "\n",
    "input = { \"witnesses\": [ witness_W1, witness_W2, witness_W3, witness_W4 ] }\n",
    "\n",
    "\n",
    "\n",
    "graph = collate(input, output='json', segmentation=False) \n",
    "print(graph)\n",
    "\n",
    "## !!! Probabilmente NON SERVONO n (normalized alignment) e p (normalized interpretation), ma solo l'originale (t) e quello normalizzato (n) bastano.\n",
    "## Ora da questo json dobbiamo tirare fuori una html table come la vogliamo noi, ovvero\n",
    "# rosso, quando n sono diversi\n",
    "# verde, quando n e t sono uguali\n",
    "# giallo, quando n sono uguali, ma t diversi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================= ALL THIS NOT USED ======================================\n",
    "# from the code in display module\n",
    "from collatex.HTML import Table, TableRow, TableCell\n",
    "from textwrap import fill\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def visualize_table_vertically_with_colors_CUSTOM(table, collation):\n",
    "    # print the table vertically\n",
    "    # switch columns and rows\n",
    "    rows = []\n",
    "    for column in table.columns:\n",
    "        cells = []\n",
    "        for witness in collation.witnesses:\n",
    "            cell = column.tokens_per_witness.get(witness.sigil)\n",
    "            cells.append(TableCell(text=fill(\"\".join(item.token_data[\"t\"] for item in cell) if cell else \"-\", 20), bgcolor=\"FF5000\" if column.variant else \"00FFFF\"))\n",
    "        rows.append(TableRow(cells=cells))\n",
    "    sigli = []\n",
    "    \n",
    "    for witness in collation.witnesses:\n",
    "        sigli.append(witness.sigil)\n",
    "    \n",
    "    x = Table(header_row=sigli, rows=rows)\n",
    "    print(x)\n",
    "    return display(HTML(str(x)))\n",
    "\n",
    "# table = collate(input, output='table', segmentation=False) ## graph\n",
    "# visualize_table_vertically_with_colors_CUSTOM(table, collation)\n",
    "# ==========================================================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'_sigil': 'W1', '_token_array_position': 0, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}], None, [{'_sigil': 'W1', '_token_array_position': 1, 'n': 'conte', 'p': 'conte', 't': 'conte '}], [{'_sigil': 'W1', '_token_array_position': 2, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W1', '_token_array_position': 3, 'n': 'rois', 'p': 'rois', 't': 'rois '}], [{'_sigil': 'W1', '_token_array_position': 4, 'n': 'a', 'p': 'a', 't': 'a '}], [{'_sigil': 'W1', '_token_array_position': 5, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W1', '_token_array_position': 6, 'n': 'reine', 'p': 'reine', 't': 'reine '}], [{'_sigil': 'W1', '_token_array_position': 7, 'n': 'coment', 'p': 'coment', 't': 'coment '}], [{'_sigil': 'W1', '_token_array_position': 8, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W1', '_token_array_position': 9, 'n': 'dame', 'p': 'dame', 't': 'dame '}], [{'_sigil': 'W1', '_token_array_position': 10, 'n': 'del', 'p': 'del', 't': 'del '}], [{'_sigil': 'W1', '_token_array_position': 11, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]]\n",
      "[[{'_sigil': 'W2', '_token_array_position': 13, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}], None, [{'_sigil': 'W2', '_token_array_position': 14, 'n': 'conte', 'p': 'conte', 't': 'conte '}], [{'_sigil': 'W2', '_token_array_position': 15, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W2', '_token_array_position': 16, 'n': 'rois', 'p': 'rois', 't': 'rois '}], [{'_sigil': 'W2', '_token_array_position': 17, 'n': 'a', 'p': 'a', 't': 'a '}], [{'_sigil': 'W2', '_token_array_position': 18, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W2', '_token_array_position': 19, 'n': 'reine', 'p': 'reine', 't': 'reine '}], [{'_sigil': 'W2', '_token_array_position': 20, 'n': 'coment', 'p': 'coment', 't': 'coment '}], [{'_sigil': 'W2', '_token_array_position': 21, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W2', '_token_array_position': 22, 'n': 'dame', 'p': 'dame', 't': 'dame '}], [{'_sigil': 'W2', '_token_array_position': 23, 'n': 'del', 'p': 'del', 't': 'del '}], [{'_sigil': 'W2', '_token_array_position': 24, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]]\n",
      "[[{'_sigil': 'W3', '_token_array_position': 26, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}], None, [{'_sigil': 'W3', '_token_array_position': 27, 'n': 'conte', 'p': 'conte', 't': 'conte '}], [{'_sigil': 'W3', '_token_array_position': 28, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W3', '_token_array_position': 29, 'n': 'rois', 'p': 'rois', 't': 'rois '}], [{'_sigil': 'W3', '_token_array_position': 30, 'n': 'a', 'p': 'a', 't': 'a '}], [{'_sigil': 'W3', '_token_array_position': 31, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W3', '_token_array_position': 32, 'n': 'reine', 'p': 'reine', 't': 'roine '}], [{'_sigil': 'W3', '_token_array_position': 33, 'n': 'coment', 'p': 'coment', 't': 'coment '}], [{'_sigil': 'W3', '_token_array_position': 34, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W3', '_token_array_position': 35, 'n': 'dame', 'p': 'dame', 't': 'dame '}], [{'_sigil': 'W3', '_token_array_position': 36, 'n': 'del', 'p': 'del', 't': 'del '}], [{'_sigil': 'W3', '_token_array_position': 37, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]]\n",
      "[[{'_sigil': 'W4', '_token_array_position': 39, 'n': 'Adonc', 'p': 'Adonc', 't': 'Adonc '}], [{'_sigil': 'W4', '_token_array_position': 40, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W4', '_token_array_position': 41, 'n': 'conte', 'p': 'conte', 't': 'conte '}], [{'_sigil': 'W4', '_token_array_position': 42, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W4', '_token_array_position': 43, 'n': 'rois', 'p': 'rois', 't': 'rois '}], None, None, None, [{'_sigil': 'W4', '_token_array_position': 44, 'n': 'coment', 'p': 'coment', 't': 'comment '}], [{'_sigil': 'W4', '_token_array_position': 45, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W4', '_token_array_position': 46, 'n': 'dame', 'p': 'dame', 't': 'dame '}], [{'_sigil': 'W4', '_token_array_position': 47, 'n': '', 'p': 'del', 't': 'du '}], [{'_sigil': 'W4', '_token_array_position': 48, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "data = json.loads(graph)\n",
    "for element in data['table']:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_sigil': 'W1', '_token_array_position': 0, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 13, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 26, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 39, 'n': 'Adonc', 'p': 'Adonc', 't': 'Adonc '}]\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "data = json.loads(graph)\n",
    "for row in data['table']:\n",
    "    print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lors \n",
      "Lors \n",
      "Lors \n",
      "Adonc \n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    " \n",
    "data = json.loads(graph)\n",
    "for row in data['table']:\n",
    "    for elem in row[0]: # attention, if a line with NONE it will give error\n",
    "        print(elem['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "W2\n",
      "W3\n",
      "W4\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# DATA FOR THE HEADER OF THE TABLE\n",
    "# The json is composed by 'table' and 'witnesses'\n",
    "################################################################\n",
    "\n",
    "# import json\n",
    " \n",
    "data = json.loads(graph)\n",
    "for row in data['witnesses']:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITNESS\n",
      "[{'_sigil': 'W1', '_token_array_position': 0, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "None\n",
      "[{'_sigil': 'W1', '_token_array_position': 1, 'n': 'conte', 'p': 'conte', 't': 'conte '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 2, 'n': 'li', 'p': 'li', 't': 'li '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 3, 'n': 'rois', 'p': 'rois', 't': 'rois '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 4, 'n': 'a', 'p': 'a', 't': 'a '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 5, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 6, 'n': 'reine', 'p': 'reine', 't': 'reine '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 7, 'n': 'coment', 'p': 'coment', 't': 'coment '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 8, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 9, 'n': 'dame', 'p': 'dame', 't': 'dame '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 10, 'n': 'del', 'p': 'del', 't': 'del '}]\n",
      "[{'_sigil': 'W1', '_token_array_position': 11, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]\n",
      "WITNESS\n",
      "[{'_sigil': 'W2', '_token_array_position': 13, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "None\n",
      "[{'_sigil': 'W2', '_token_array_position': 14, 'n': 'conte', 'p': 'conte', 't': 'conte '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 15, 'n': 'li', 'p': 'li', 't': 'li '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 16, 'n': 'rois', 'p': 'rois', 't': 'rois '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 17, 'n': 'a', 'p': 'a', 't': 'a '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 18, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 19, 'n': 'reine', 'p': 'reine', 't': 'reine '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 20, 'n': 'coment', 'p': 'coment', 't': 'coment '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 21, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 22, 'n': 'dame', 'p': 'dame', 't': 'dame '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 23, 'n': 'del', 'p': 'del', 't': 'del '}]\n",
      "[{'_sigil': 'W2', '_token_array_position': 24, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]\n",
      "WITNESS\n",
      "[{'_sigil': 'W3', '_token_array_position': 26, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}]\n",
      "None\n",
      "[{'_sigil': 'W3', '_token_array_position': 27, 'n': 'conte', 'p': 'conte', 't': 'conte '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 28, 'n': 'li', 'p': 'li', 't': 'li '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 29, 'n': 'rois', 'p': 'rois', 't': 'rois '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 30, 'n': 'a', 'p': 'a', 't': 'a '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 31, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 32, 'n': 'reine', 'p': 'reine', 't': 'roine '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 33, 'n': 'coment', 'p': 'coment', 't': 'coment '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 34, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 35, 'n': 'dame', 'p': 'dame', 't': 'dame '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 36, 'n': 'del', 'p': 'del', 't': 'del '}]\n",
      "[{'_sigil': 'W3', '_token_array_position': 37, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]\n",
      "WITNESS\n",
      "[{'_sigil': 'W4', '_token_array_position': 39, 'n': 'Adonc', 'p': 'Adonc', 't': 'Adonc '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 40, 'n': 'li', 'p': 'li', 't': 'li '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 41, 'n': 'conte', 'p': 'conte', 't': 'conte '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 42, 'n': 'li', 'p': 'li', 't': 'li '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 43, 'n': 'rois', 'p': 'rois', 't': 'rois '}]\n",
      "None\n",
      "None\n",
      "None\n",
      "[{'_sigil': 'W4', '_token_array_position': 44, 'n': 'coment', 'p': 'coment', 't': 'comment '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 45, 'n': 'la', 'p': 'la', 't': 'la '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 46, 'n': 'dame', 'p': 'dame', 't': 'dame '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 47, 'n': '', 'p': 'del', 't': 'du '}]\n",
      "[{'_sigil': 'W4', '_token_array_position': 48, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    " \n",
    "dataTestIn = json.loads(graph)\n",
    "for x in dataTestIn['table']:\n",
    "    print(\"WITNESS\")\n",
    "    for y in x:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"cell\": [{\"_sigil\": \"W1\", \"_token_array_position\": 11, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]}, {\"cell\": [{\"_sigil\": \"W2\", \"_token_array_position\": 24, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]}, {\"cell\": [{\"_sigil\": \"W3\", \"_token_array_position\": 37, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]}, {\"cell\": [{\"_sigil\": \"W4\", \"_token_array_position\": 48, \"n\": \"lac\", \"p\": \"lac\", \"t\": \"lac\\n\"}]}]\n"
     ]
    }
   ],
   "source": [
    "## TRANSFORMING COLLATEX JSON OUTPUT INTO ANOTHER JSON, MORE SUITABLE FOR GENERATING HTML TABLE (see below: ideal json ..)\n",
    "# import json\n",
    " \n",
    "dataTestIn = json.loads(graph)\n",
    "\n",
    "\n",
    "dataTestOut = []\n",
    "for x in dataTestIn['table']:\n",
    "    item = {}\n",
    "    for y in x:\n",
    "        if (y != None):\n",
    "            item = {\"cell\": y}\n",
    "    dataTestOut.append(item)\n",
    "    \n",
    "    ## esce solo l'ultima !!!!\n",
    "    ## guarda:   https://stackoverflow.com/questions/13530967/parsing-data-to-create-a-json-data-object-with-python\n",
    "    \n",
    " \n",
    "json_data = json.dumps(dataTestOut)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'_sigil': 'W1', '_token_array_position': 0, 'n': 'Lors', 'p': 'Lors', 't': 'Lors '}], None, [{'_sigil': 'W1', '_token_array_position': 1, 'n': 'conte', 'p': 'conte', 't': 'conte '}], [{'_sigil': 'W1', '_token_array_position': 2, 'n': 'li', 'p': 'li', 't': 'li '}], [{'_sigil': 'W1', '_token_array_position': 3, 'n': 'rois', 'p': 'rois', 't': 'rois '}], [{'_sigil': 'W1', '_token_array_position': 4, 'n': 'a', 'p': 'a', 't': 'a '}], [{'_sigil': 'W1', '_token_array_position': 5, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W1', '_token_array_position': 6, 'n': 'reine', 'p': 'reine', 't': 'reine '}], [{'_sigil': 'W1', '_token_array_position': 7, 'n': 'coment', 'p': 'coment', 't': 'coment '}], [{'_sigil': 'W1', '_token_array_position': 8, 'n': 'la', 'p': 'la', 't': 'la '}], [{'_sigil': 'W1', '_token_array_position': 9, 'n': 'dame', 'p': 'dame', 't': 'dame '}], [{'_sigil': 'W1', '_token_array_position': 10, 'n': 'del', 'p': 'del', 't': 'del '}], [{'_sigil': 'W1', '_token_array_position': 11, 'n': 'lac', 'p': 'lac', 't': 'lac\\n'}]]\n"
     ]
    }
   ],
   "source": [
    "## TEST\n",
    "# import json\n",
    " \n",
    "data = json.loads(graph)\n",
    "print(data['table'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECT TEST:   Lors\n",
      "ROW\n",
      "Lors\n",
      "Lors\n",
      "Lors\n",
      "Adonc\n",
      "ROW\n",
      "conte\n",
      "conte\n",
      "conte\n",
      "conte\n"
     ]
    }
   ],
   "source": [
    "## IDEAL JSON FOR GENERATING THE HTML TABLE\n",
    "\n",
    "import json\n",
    "testGraph = '''{\n",
    "\t\"table\": [\n",
    "\t\t[\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W1\",\n",
    "\t\t\t\t\"_token_array_position\": 0,\n",
    "\t\t\t\t\"n\": \"Lors\",\n",
    "\t\t\t\t\"p\": \"Lors\",\n",
    "\t\t\t\t\"t\": \"Lors \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W2\",\n",
    "\t\t\t\t\"_token_array_position\": 13,\n",
    "\t\t\t\t\"n\": \"Lors\",\n",
    "\t\t\t\t\"p\": \"Lors\",\n",
    "\t\t\t\t\"t\": \"Lors \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W3\",\n",
    "\t\t\t\t\"_token_array_position\": 26,\n",
    "\t\t\t\t\"n\": \"Lors\",\n",
    "\t\t\t\t\"p\": \"Lors\",\n",
    "\t\t\t\t\"t\": \"Lors \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W4\",\n",
    "\t\t\t\t\"_token_array_position\": 39,\n",
    "\t\t\t\t\"n\": \"Adonc\",\n",
    "\t\t\t\t\"p\": \"Adonc\",\n",
    "\t\t\t\t\"t\": \"Adonc \"\n",
    "\t\t\t}]\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W1\",\n",
    "\t\t\t\t\"_token_array_position\": 1,\n",
    "\t\t\t\t\"n\": \"conte\",\n",
    "\t\t\t\t\"p\": \"conte\",\n",
    "\t\t\t\t\"t\": \"conte \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W2\",\n",
    "\t\t\t\t\"_token_array_position\": 14,\n",
    "\t\t\t\t\"n\": \"conte\",\n",
    "\t\t\t\t\"p\": \"conte\",\n",
    "\t\t\t\t\"t\": \"conte \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W3\",\n",
    "\t\t\t\t\"_token_array_position\": 27,\n",
    "\t\t\t\t\"n\": \"conte\",\n",
    "\t\t\t\t\"p\": \"conte\",\n",
    "\t\t\t\t\"t\": \"conte \"\n",
    "\t\t\t}],\n",
    "\t\t\t[{\n",
    "\t\t\t\t\"_sigil\": \"W4\",\n",
    "\t\t\t\t\"_token_array_position\": 41,\n",
    "\t\t\t\t\"n\": \"conte\",\n",
    "\t\t\t\t\"p\": \"conte\",\n",
    "\t\t\t\t\"t\": \"conte \"\n",
    "\t\t\t}]\n",
    "\t\t]\n",
    "\t],\n",
    "\t\"witnesses\": [\n",
    "\t\t\"W1\",\n",
    "\t\t\"W2\",\n",
    "\t\t\"W3\",\n",
    "\t\t\"W4\"\n",
    "\t]\n",
    "}'''\n",
    "\n",
    "testData = json.loads(testGraph)\n",
    "\n",
    "print(\"DIRECT TEST:   \"+testData['table'][0][0][0]['n'])\n",
    "\n",
    "for row in testData['table']:\n",
    "    print(\"ROW\")\n",
    "    for cellList in row:\n",
    "        for cellDic in cellList:\n",
    "            print(cellDic['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    " \n",
    "\n",
    "# DATA FROM PREVIOUS CELL\n",
    "\n",
    "## open table and thead\n",
    "html = \"\"\"<html><table border=\"1\"><thead><tr>\"\"\"\n",
    "for witness in testData['witnesses']:   \n",
    "    html += \"<th>\"+witness+\"</th>\"\n",
    "## close thead\n",
    "html += \"</tr></thead>\"  \n",
    "\n",
    "## iterate over \"rows\"\n",
    "for row in testData['table']:\n",
    "    ## open tbody\n",
    "    html += \"<tbody><tr>\"\n",
    "    ## iterate over elements inside \"rows\"\n",
    "    for cellList in row:\n",
    "        for cellDic in cellList:\n",
    "            normToken = cellDic['n']\n",
    "            html += \"<td>\"+normToken+\"</td>\"\n",
    "    ## close tbody\n",
    "    html += \"</tr></tbody>\"\n",
    "file_ = open('result.html', 'w')\n",
    "file_.write(html)\n",
    "file_.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
