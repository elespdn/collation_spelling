{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary, where the original form stays at it is and the normalized form is: POS_LEMMA. For example\n",
    "\n",
    "est, VER_estre\n",
    "\n",
    "les, ART_le\n",
    "\n",
    "\n",
    "The POS tagging and lemmatization is here perfermed with TreeTaggerWrapper (a TreeTagger Python version), using the parameters for Old French made available by Achim Stein.\n",
    "\n",
    "## Function for POS tagging and lemma annotation, plus some cleaning, of the text of all witnesses\n",
    "\n",
    "### N.b.: change the parameter below (where the function is called) to create the dictionary for one particular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import treetaggerwrapper, os, re\n",
    "\n",
    "\n",
    "def create_poslemma(example): \n",
    "\n",
    "\n",
    "    # create a folder where to store intermediate results.\n",
    "    try:\n",
    "        os.makedirs('data/' + example + '/process')\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # put together the texts of all the witnesses\n",
    "    with open('data/' + example + '/W1.txt') as W1, \\\n",
    "         open('data/' + example + '/W2.txt') as W2, \\\n",
    "         open('data/' + example + '/W3.txt') as W3, \\\n",
    "         open('data/' + example + '/process/p1_all_texts.txt', 'w', encoding='utf-8') as all_texts:\n",
    "        all_texts.write(W1.read() + ' ' + W2.read() + ' ' + W3.read())\n",
    "\n",
    "    # each word a new line, in alphabetical order: sorted()\n",
    "    infile = open('data/' + example + '/process/p1_all_texts.txt').read()\n",
    "    outfile = open('data/' + example + '/process/p2_all_words.txt', 'w', encoding='utf-8')\n",
    "    words = infile.split()\n",
    "    for word in sorted(words):\n",
    "        outfile.write(word + '\\n')\n",
    "\n",
    "    # delete duplicates\n",
    "    infile = 'data/' + example + '/process/p2_all_words.txt'\n",
    "    lines_seen = set() # holds lines already seen\n",
    "    outfile = open('data/' + example + '/process/p3_single_words.txt', 'w', encoding='utf-8')\n",
    "    for line in open(infile, \"r\"):\n",
    "        if line not in lines_seen: # not a duplicate\n",
    "            outfile.write(line)\n",
    "            lines_seen.add(line)\n",
    "    outfile.close()\n",
    "\n",
    "    # pos and lemma tagging\n",
    "    infile = 'data/' + example + '/process/p3_single_words.txt'\n",
    "    outfile = 'data/' + example + '/process/p4_single_words_analyzed.txt'\n",
    "    open(outfile, 'w', encoding='utf-8')\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='stein')\n",
    "    tagger.tag_file_to(infile, outfile)\n",
    "\n",
    "\n",
    "    # clean pos and lemma tagging, keeping only the first lemma suggested\n",
    "    infile = 'data/' + example + '/process/p4_single_words_analyzed.txt'\n",
    "    outfile = 'data/' + example + '/process/p5_single_words_analyzed_clean.txt'\n",
    "    patterns = [('_.*', ''),\n",
    "                ('\\d.*', ''),\n",
    "    ##            ('\\|.*', ''), so that the different output possibilities are saved\n",
    "                ('�', 'ö'),  ## encoding problem, but it does not seem to depend on the TreeTaggerWrapper, nor on the script. Maybe on the lexicon? Anyway, this is not real solution but works\n",
    "                ('<nolem>', 'UNKNOWN')]\n",
    "    t = open(infile).read()\n",
    "    for (p1,p2) in patterns:\n",
    "        p = re.compile(p1)\n",
    "        t = p.sub(p2, t)\n",
    "    o = open(outfile, 'w', encoding='utf-8')\n",
    "    o.write(t)\n",
    "    o.close()\n",
    "\n",
    "\n",
    "    # create pos_lemma.csv\n",
    "    infile = open('data/' + example + '/process/p5_single_words_analyzed_clean.txt', 'r')\n",
    "    outfile = open('pos_lemma_' + example + '.csv', 'w', encoding='utf-8')\n",
    "    for aline in infile:\n",
    "        values = aline.split()\n",
    "        original = values[0]\n",
    "        pos = values[1]\n",
    "        lemmas = values[2]\n",
    "        print(original + ',' + pos + '_' + lemmas + '\\n')   ## print them, in order to have immediately check\n",
    "        outfile.write(original + ',' + pos + '_' + lemmas + '\\n')   ## print them on file\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the function with the number of the example as parameter (ex. 'example1'). \n",
    "\n",
    "The result will be printed both here (or in the terminal) and on the output file with the corresponding name (ex. 'pos_lemma_example1.csv')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atorner,VER_atorner\n",
      "\n",
      "deit,VER_devoir|dire\n",
      "\n",
      "doit,VER_devoir|dire\n",
      "\n",
      "li,PRO:pers_ele|il\n",
      "\n",
      "mie,ADV_mie\n",
      "\n",
      "ne,PROCON_ni\n",
      "\n",
      "on,PRO:invar_on\n",
      "\n",
      "pas,NOM_pas|past\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_poslemma('example11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
