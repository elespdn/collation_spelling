{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create taggedDistinct from xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taggedDistinct created!\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "import csv\n",
    "import os\n",
    "\n",
    "with open('dictionaries/taggedAll_example2.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # for all the xml files in the dir\n",
    "    path = 'data/example2'\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith('.xml'): continue\n",
    "        fullname = os.path.join(path, filename)\n",
    "        \n",
    "        # open them\n",
    "        xmldata = open(fullname, encoding='utf-8' ).read()\n",
    "        \n",
    "        #parse them\n",
    "        root = ET.fromstring(xmldata)\n",
    "        body = root[1][0]\n",
    "        for l in body:\n",
    "            \n",
    "            # for all chilf of l, if they are w\n",
    "            for w in l:\n",
    "                if (w.tag == '{http://www.tei-c.org/ns/1.0}w'):\n",
    "                    \n",
    "                    wtext = w.text\n",
    "                    wtype = w.get('type')\n",
    "                    wlemma = w.get('lemma')\n",
    "                    wtypelemma = wtype + '_' + wlemma\n",
    "\n",
    "                    writer.writerow([wtext,wtypelemma])\n",
    "f.close\n",
    "\n",
    "\n",
    "# from taggedAll to taggedDistinct                  \n",
    "reader=csv.reader(open('dictionaries/taggedAll_example2.csv', 'r'), delimiter=',')\n",
    "with open('dictionaries/taggedDistinct_example2.csv', 'w', newline='') as o:\n",
    "    writer = csv.writer(o)\n",
    "    entries = set()\n",
    "    writer.writerow(['Original', 'Normalised'])\n",
    "    for row in reader:\n",
    "        key = (row[0], row[1]) \n",
    "        if key not in entries:\n",
    "            writer.writerow(row)\n",
    "            entries.add(key)\n",
    "o.close\n",
    "\n",
    "print(\"taggedDistinct created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract text from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A lui meismes se consoille Auquel d aus ii il aidera Lors dit qu au lyon se tanra Qu a venimeus ne a felon Ne doit an feire se mal non\n",
      "A li meisme se conselle Alquel des ii il aidera Et dist au lion se tendra Qu a venimeus et a felon Ne doit l on faire se mal non\n",
      "A lui meismes se conseille Auquel des deuz il aidera Lor dist c au lyon aidera Qu a enuious et a felon Ne doit on faire se mal non\n",
      "A lui meismes se conseille Auquel des ii aidera De la part au lyon sera Qu a nul mauves ne a felon Ne doit on fere se mal non\n",
      "A lui meisme se conseille Auquel de ii il aidera Il dist c au lion secourra Car a vrinmeus n a felon Ne doit on faire se mal non\n",
      "A soi meismes se conseille Auquel des ii il aidera Lors dit au lyon le fera C a venimeus et a felon Ne doit l en fere se mal non\n",
      "A soi meisme se conselle Alquel des ii il aidera Lors dist c al lion secolra C a venimex ne a felon Ne doit on faire se mal non\n",
      "A lui meisme se consele As ques des dex il aidera Dist que le lion secoura Qu as venimeus ne as felons Ne doit on faire se mal non\n",
      "A lui meismes se conselle Auquel des ii il aidera Et dit q au leon secorra Q a venimeus et a felon Ne doit l en fere se mal non\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "path = 'data/example2'\n",
    "for filename in os.listdir(path):\n",
    "    if not filename.endswith('.xml'): continue\n",
    "    xmlfile = os.path.join(path, filename)\n",
    "    \n",
    "    txtfile = xmlfile.split(\".\")[0]+\".txt\"\n",
    "    \n",
    "    xmldata = open(xmlfile, encoding='utf-8' ).read()\n",
    "    \n",
    "    root = ET.fromstring(xmldata)\n",
    "    body = root[1][0]\n",
    "    listw = []\n",
    "    \n",
    "    for l in body:\n",
    "        # for each child of l, if it is w\n",
    "        for w in l:\n",
    "            if (w.tag == '{http://www.tei-c.org/ns/1.0}w'):\n",
    "                wtext = w.text\n",
    "                listw.append(wtext)\n",
    "    text = \" \".join(listw)\n",
    "                \n",
    "    with open (txtfile, 'w') as o:\n",
    "        o.write(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare text for next collation: all to lower cases, strip punctuation and apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## All to lower cases and strip punctuation and apostrophes\n",
    "## this goes file per file. To do: all files together\n",
    "\n",
    "from itertools import chain\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "with open('data/example1original/V.txt', 'r') as file:\n",
    "    file = file.read()\n",
    "    \n",
    "    file = re.sub('\\.','',re.sub(\"'\",' ',file))\n",
    "\n",
    "lines = [line.lower() for line in file]\n",
    "\n",
    "with open('data/example1/V.txt', 'w') as out:\n",
    "     out.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "import csv, re\n",
    "from general_functions import tag_poslemma\n",
    "from general_functions import table_automaticDictionary\n",
    "\n",
    "\n",
    "tag_poslemma('example2')  # ex: create_poslemma('example2')\n",
    "print(\"taggedAll and taggedDistinct created in folder Dictionaries/ !\")\n",
    "\n",
    "\n",
    "A = open( \"data/example2/A.txt\", encoding='utf-8' ).read()\n",
    "F = open( \"data/example2/F.txt\", encoding='utf-8' ).read()\n",
    "G = open( \"data/example2/G.txt\", encoding='utf-8' ).read()\n",
    "H = open( \"data/example2/H.txt\", encoding='utf-8' ).read()\n",
    "\n",
    "#create the dictionary consisting of two columns, separated by a comma.\n",
    "# The first Column 'Original' are the strings as found in the text, \n",
    "# the second column 'Normalised' contains the strings you want to replace them with. \n",
    "# No whitespaces behind the strings.\n",
    "Normit = {}\n",
    "with open('dictionaries/taggedDistinct_example2.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'Normalised'])\n",
    "    for row in reader:\n",
    "        Normit[row['Original']]= row['Normalised']\n",
    "\n",
    "#read in the witnesses  from your file system \n",
    "from collatex.core_classes import WordPunctuationTokenizer\n",
    "tokenizer = WordPunctuationTokeni\"zer()\n",
    "\n",
    "# build a function to tokenize and to normalize by replace keys to be \n",
    "# found in the dictionary by the corresponding values \n",
    "def tokennormalizer(witness) :\n",
    "    tokens_as_strings = tokenizer.tokenize(witness)\n",
    "    list = []\n",
    "    for token_string in tokens_as_strings:\n",
    "        normversion = re.sub(r'\\s+$',\"\", token_string)\n",
    "        replaceversion = Normit.get(normversion,normversion)\n",
    "        list.append({'t':token_string, 'n':replaceversion})\n",
    "    return(list)\n",
    "\n",
    "#collate\n",
    "tokens_A = tokennormalizer(A) \n",
    "tokens_F = tokennormalizer(F) \n",
    "tokens_G = tokennormalizer(G) \n",
    "tokens_H = tokennormalizer(H) \n",
    "\n",
    "witness_A = { \"id\": \"W1\", \"tokens\":tokens_A }\n",
    "witness_F = { \"id\": \"W2\", \"tokens\":tokens_F }\n",
    "witness_G = { \"id\": \"W3\", \"tokens\":tokens_G }\n",
    "witness_H = { \"id\": \"W4\", \"tokens\":tokens_H }\n",
    "\n",
    "\n",
    "input = { \"witnesses\": [ witness_A, witness_F, witness_G, witness_H ] }\n",
    "\n",
    "\n",
    "table = collate(input, output='html2', segmentation=False)\n",
    "graphSvg = collate(input, output='svg', segmentation=False)\n",
    "\n",
    "graph_automaticDictionary = collate(input, output='json', segmentation=False)\n",
    "table_automaticDictionary(graph_automaticDictionary, 'example2')\n",
    "print('external table created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "import csv, re\n",
    "from general_functions import tag_poslemma\n",
    "from general_functions import table_automaticDictionary\n",
    "\n",
    "\n",
    "tag_poslemma('example2')  # ex: create_poslemma('example2')\n",
    "print(\"taggedAll and taggedDistinct created in folder Dictionaries/ !\")\n",
    "\n",
    "M = open( \"data/example2/M.txt\", encoding='utf-8' ).read()\n",
    "P = open( \"data/example2/P.txt\", encoding='utf-8' ).read()\n",
    "R = open( \"data/example2/R.txt\", encoding='utf-8' ).read()\n",
    "S = open( \"data/example2/S.txt\", encoding='utf-8' ).read()\n",
    "\n",
    "\n",
    "#create the dictionary consisting of two columns, separated by a comma.\n",
    "# The first Column 'Original' are the strings as found in the text, \n",
    "# the second column 'Normalised' contains the strings you want to replace them with. \n",
    "# No whitespaces behind the strings.\n",
    "Normit = {}\n",
    "with open('dictionaries/taggedDistinct_example2.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'Normalised'])\n",
    "    for row in reader:\n",
    "        Normit[row['Original']]= row['Normalised']\n",
    "\n",
    "#read in the witnesses  from your file system \n",
    "from collatex.core_classes import WordPunctuationTokenizer\n",
    "tokenizer = WordPunctuationTokenizer()\n",
    "\n",
    "# build a function to tokenize and to normalize by replace keys to be \n",
    "# found in the dictionary by the corresponding values \n",
    "def tokennormalizer(witness) :\n",
    "    tokens_as_strings = tokenizer.tokenize(witness)\n",
    "    list = []\n",
    "    for token_string in tokens_as_strings:\n",
    "        normversion = re.sub(r'\\s+$',\"\", token_string)\n",
    "        replaceversion = Normit.get(normversion,normversion)\n",
    "        list.append({'t':token_string, 'n':replaceversion})\n",
    "    return(list)\n",
    "\n",
    "#collate\n",
    "tokens_M = tokennormalizer(M) \n",
    "tokens_P = tokennormalizer(P) \n",
    "tokens_R = tokennormalizer(R) \n",
    "tokens_S = tokennormalizer(S) \n",
    "\n",
    "witness_M = { \"id\": \"W1\", \"tokens\":tokens_M }\n",
    "witness_P = { \"id\": \"W2\", \"tokens\":tokens_P }\n",
    "witness_R = { \"id\": \"W3\", \"tokens\":tokens_R }\n",
    "witness_S = { \"id\": \"W4\", \"tokens\":tokens_S }\n",
    "\n",
    "\n",
    "input = { \"witnesses\": [ witness_M, witness_P, witness_R, witness_S ] }\n",
    "\n",
    "\n",
    "\n",
    "table = collate(input, output='html2', segmentation=False)\n",
    "graphSvg = collate(input, output='svg', segmentation=False)\n",
    "\n",
    "graph_automaticDictionary = collate(input, output='json', segmentation=False)\n",
    "table_automaticDictionary(graph_automaticDictionary, 'example2')\n",
    "print('external table created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "import csv, re\n",
    "from general_functions import tag_poslemma\n",
    "from general_functions import table_automaticDictionary\n",
    "\n",
    "\n",
    "tag_poslemma('example2')  # ex: create_poslemma('example2')\n",
    "print(\"taggedAll and taggedDistinct created in folder Dictionaries/ !\")\n",
    "\n",
    "\n",
    "A = open( \"data/example2/A.txt\", encoding='utf-8' ).read()\n",
    "F = open( \"data/example2/F.txt\", encoding='utf-8' ).read()\n",
    "G = open( \"data/example2/G.txt\", encoding='utf-8' ).read()\n",
    "H = open( \"data/example2/H.txt\", encoding='utf-8' ).read()\n",
    "M = open( \"data/example2/M.txt\", encoding='utf-8' ).read()\n",
    "P = open( \"data/example2/P.txt\", encoding='utf-8' ).read()\n",
    "R = open( \"data/example2/R.txt\", encoding='utf-8' ).read()\n",
    "S = open( \"data/example2/S.txt\", encoding='utf-8' ).read()\n",
    "V = open( \"data/example2/V.txt\", encoding='utf-8' ).read()\n",
    "\n",
    "\n",
    "#create the dictionary consisting of two columns, separated by a comma.\n",
    "# The first Column 'Original' are the strings as found in the text, \n",
    "# the second column 'Normalised' contains the strings you want to replace them with. \n",
    "# No whitespaces behind the strings.\n",
    "Normit = {}\n",
    "with open('dictionaries/taggedDistinct_example2.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'Normalised'])\n",
    "    for row in reader:\n",
    "        Normit[row['Original']]= row['Normalised']\n",
    "\n",
    "#read in the witnesses  from your file system \n",
    "from collatex.core_classes import WordPunctuationTokenizer\n",
    "tokenizer = WordPunctuationTokenizer()\n",
    "\n",
    "# build a function to tokenize and to normalize by replace keys to be \n",
    "# found in the dictionary by the corresponding values \n",
    "def tokennormalizer(witness) :\n",
    "    tokens_as_strings = tokenizer.tokenize(witness)\n",
    "    list = []\n",
    "    for token_string in tokens_as_strings:\n",
    "        normversion = re.sub(r'\\s+$',\"\", token_string)\n",
    "        replaceversion = Normit.get(normversion,normversion)\n",
    "        list.append({'t':token_string, 'n':replaceversion})\n",
    "    return(list)\n",
    "\n",
    "tokens_A = tokennormalizer(A) \n",
    "tokens_F = tokennormalizer(F) \n",
    "tokens_G = tokennormalizer(G) \n",
    "tokens_H = tokennormalizer(H) \n",
    "tokens_M = tokennormalizer(M) \n",
    "tokens_P = tokennormalizer(P) \n",
    "tokens_R = tokennormalizer(R) \n",
    "tokens_S = tokennormalizer(S) \n",
    "tokens_V = tokennormalizer(V) \n",
    "\n",
    "witness_A = { \"id\": \"W1\", \"tokens\":tokens_A }\n",
    "witness_F = { \"id\": \"W2\", \"tokens\":tokens_F }\n",
    "witness_G = { \"id\": \"W3\", \"tokens\":tokens_G }\n",
    "witness_H = { \"id\": \"W4\", \"tokens\":tokens_H }\n",
    "witness_M = { \"id\": \"W1\", \"tokens\":tokens_M }\n",
    "witness_P = { \"id\": \"W2\", \"tokens\":tokens_P }\n",
    "witness_R = { \"id\": \"W3\", \"tokens\":tokens_R }\n",
    "witness_S = { \"id\": \"W4\", \"tokens\":tokens_S }\n",
    "witness_V = { \"id\": \"W1\", \"tokens\":tokens_V }\n",
    "\n",
    "\n",
    "input = { \"witnesses\": [ witness_A, witness_F, witness_G, witness_H, witness_M, witness_P, witness_R, witness_S, witness_V ] }\n",
    "\n",
    "\n",
    "\n",
    "table = collate(input, output='html2', segmentation=False)\n",
    "graphSvg = collate(input, output='svg', segmentation=False)\n",
    "\n",
    "graph_automaticDictionary = collate(input, output='json', segmentation=False)\n",
    "table_automaticDictionary(graph_automaticDictionary, 'example2')\n",
    "print('external table created!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
